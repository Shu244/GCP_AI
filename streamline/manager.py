import gcp_interactions as gcp
import argparse
import strings
import random
import torch
import copy
import time
import json
import os


'''
Best model and VM progress will save the same items:
-hyparameters, parameters, progress report

results will save one file containing:
-hyperparameters and progress report
'''


'''
Triggers saving the current state of the model, hyperparameters, and performance
'''
class Manager:
    def __init__(self, temp_path, bucket_name, rank):
        self.rank = rank
        self.bucket_name = bucket_name
        self.temp_path = temp_path
        self.quick_send = gcp.QuickSend(temp_path, bucket_name)

        self.download_progress_folder(bucket_name, temp_path, rank)

        self.tracker = Tracker(self.quick_send, rank)
        self.hyparams = Hyperparameters(self.quick_send, rank)
        self.count = 0

    def download_progress_folder(self, bucket_name, tmp_folder, rank):
        folder_path = strings.vm_progress + ("/%d/" % rank)
        gcp.download_folder(bucket_name, folder_path, tmp_folder)

    def add_progress(self, key, value):
        self.tracker.add(key, value)

    def finished(self, param_dict):
        self.save_results()
        self.save_best(param_dict)
        self.reset()
        self.reset_cloud_progress()

    def reset_cloud_progress(self):
        '''
        Resets the cloud folder keeping track of progress by deleting the params and removing current hyperparameter
        values
        '''
        cloud_folder_path = os.path.join(strings.vm_progress, self.rank)
        gcp.delete_all_prefixes(self.bucket_name, cloud_folder_path)

        hyparams_copy = copy.deepcopy(self.hyparams.raw_hyparams)
        hyparams_copy.pop("current_values", None)

        self.quick_send.write(strings.vm_hyparams_report, json.dumps(hyparams_copy), cloud_folder_path)

    def reset(self):
        self.tracker.reset()
        self.hyparams.reset()

    def save_progress(self, param_dict):
        folder_path = strings.vm_progress + ("/%d" % self.rank)
        self.tracker.save_progress(folder_path)
        self.hyparams.save_hyparams(folder_path)
        self.save_params(param_dict, folder_path)

    def save_results(self):
        progress_report = self.tracker.get_report()
        hyparams_report = self.hyparams.get_raw_hyparams()

        timestr = time.strftime("%m%d%Y-%H%M%S")
        readable_timestr = time.strftime("%m/%d/%Y-%H:%M:%S")

        filename = timestr + ("-vm%d" % self.rank) + ('-%d' % self.count) + ".json"
        result = {
            "progress": progress_report,
            "hyparams": hyparams_report,
            "time": readable_timestr
        }
        msg = json.dumps(result)

        self.quick_send.send(filename, msg, strings.results)
        self.count += 1

    def save_best(self, param_dict):
        if self.isBest(self.tracker.get_report()):
            folder_path = strings.best_model + ("/%d" % self.rank)
            self.tracker.save_progress(folder_path)
            self.hyparams.save_hyparams(folder_path)
            self.save_params(param_dict, folder_path)
            return True
        return False

    def isBest(self, cur_report):
        report_path = os.path.join(strings.best_model, self.rank, strings.vm_progress_report)
        gcp.download_file(self.bucket_name, report_path, self.temp_path)

        local_report_path = os.path.join(self.temp_path, strings.vm_progress_report)
        best_report = json.load(open(local_report_path))

        # metric to compare
        compare = cur_report["compare"]
        goal = cur_report["goal"]

        if goal == "max":
            best_val = max(best_report[compare])
            cur_val = max(cur_report[compare])
            if cur_val > best_val:
                return True
        else:
            best_val = min(best_report[compare])
            cur_val = min(cur_report[compare])
            if cur_val < best_val:
                return True
        return False

    def save_params(self, param_dict, folder):
        '''
        Saving the parameters for a model to a folder determined by whether or not the training is done.

        :param param_dict: Generated by model.state_dict()
        '''
        path = os.path.join(self.temp_path, strings.params_file)
        torch.save(param_dict, path)
        gcp.upload_file(self.bucket_name, path, folder)


'''
Used to track the performance of a model while training:
-Loads and saves model progress
'''
class Tracker:
    def __init__(self, quick_send, rank):
        self.quick_send = quick_send
        self.progress_report_local_pth = os.path.join(
            quick_send.temp_path,
            rank,
            strings.vm_progress_report)
        self.report = json.load(open(self.progress_report_local_pth)) \
            if os.path.isfile(self.progress_report_local_pth) \
            else {}
        self.report["goal"] = "max"
        self.report["comapre"] = "val_accuracy"
        self.rank = rank

    def add(self, key, value):
        if key not in self.report:
            self.report[key] = []
        self.report[key].append(value)

    def save_progress(self, folder):
        self.quick_send.write(strings.vm_progress_report, json.dumps(self.report), folder)

    def get_report(self):
        return self.report

    def reset(self):
        self.report = {}
        self.report["goal"] = "max"
        self.report["comapre"] = "val_accuracy"


'''
Manages the hyperparameters:
-Loads hyperparameters and saves them
'''
class Hyperparameters:

    def __init__(self, quick_send, rank):
        file_path = os.path.join(quick_send.temp_path, strings.vm_hyparams_report)
        self.raw_hyparams = json.load(open(file_path))
        self.cur_val = "current_values"
        self.rank = rank

        if self.cur_val in self.raw_hyparams and self.raw_hyparams[self.cur_val] != None:
            # load in values
            self.cur_hyparams = self.raw_hyparams[self.cur_val]
            self.load_params = True
        else:
            # generate new values
            # self.load_params = False set in the generate method
            # self.cur_hyparams is also set
            self.reset()

    def reset(self):
        '''
        Generates new hyperparameters according to specifications in hyparam_obj
        '''

        self.load_params = False
        hyparam_copy = copy.deepcopy(self.raw_hyparams["hyperparameters"])
        for key, value in hyparam_copy.items():
            if isinstance(value, list):
                new_val = random.uniform(value[0], value[1])
                hyparam_copy[key] = new_val
        self.cur_hyparams = hyparam_copy
        self.raw_hyparams[self.cur_val] = self.cur_hyparams

    def get_hyparams(self):
        return self.cur_hyparams

    def get_raw_hyparams(self):
        '''
        Raw hyperparameter dictionary contains the current hyperparemter values as
        well as the information specifying the portion of the hyperparameter grid being searched.

        :return: Raw hyperparameter dictionary
        '''

        return self.raw_hyparams

    def save_hyparams(self, folder):
        self.quick_send.write(strings.vm_hyparams_report, json.dumps(self.raw_hyparams), folder)


def hyparam_search(manager):
    start = manager.hyparams.raw_hyparams["current_iter"]
    end = manager.hyparams.raw_hyparams["max_iter"]
    quick_send = manager.quick_send
    rank = manager.rank
    temp_path = manager.temp_path

    while start < end:
        try:
            param_pth = os.path.join(temp_path, strings.params_file) if manager.hyparams.load_params else None
            train(manager, param_pth)
        except Exception as err:
            timestr = time.strftime("%m%d%Y-%H%M%S")
            readable_timestr = time.strftime("%m/%d/%Y-%H:%M:%S")
            filename = timestr + ("-vm%d" % rank) + "-iter" + str(start) + ".json"
            msg = {
                "error": str(err),
                "hyperparameters": manager.hyparams.get_hyparams(),
                "progress": manager.tracker.get_report(),
                "time": readable_timestr
            }
            msg = json.dumps(msg)
            quick_send.send(filename, msg, strings.shared_errors)
            print("Writing the following msg to shared errors folder in Google cloud")
            print(msg)
        start += 1


def train(manager, params_path):
    print("training")
    print(params_path)
    if not params_path:
        raise Exception("Something terrible happened")
    manager.finished()


if __name__ == '__main__':
    parser = argparse.ArgumentParser(description="Training model over a portion of the hyperparameters")

    parser.add_argument('rank', help='The id for this virtual machine', type=int)
    parser.add_argument('bucket_name', help='The name of the bucket')
    parser.add_argument("-m", '--tmppth', default="./tmp", help='The folder to store temporary files before moving to gcloud')

    args = parser.parse_args()

    manager = Manager(args.tmppth, args.bucket_name, args.rank)
    hyparam_search(manager)
